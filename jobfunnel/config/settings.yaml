# This is the default settings file. Do not edit.

# all paths are relative to this file

# paths
output_path: 'search'

# providers from which to search (case insensitive)
providers:
  - 'Indeed'
  - 'Monster'
  - 'GlassDoor'

# filters
search_terms:
  region:
    province: 'ON'
    city:     'waterloo'
    domain:   'ca'
    radius:   25

  keywords:
    - 'Python'

# black-listed company names
black_list:
  - 'Infox Consulting'

# logging level options are: critical, error, warning, info, debug, notset
log_level: 'info'

# The oldest number of days a job can be. To disable this option set value to -1
#TODO make this a CLI option so it can be optional
threshold_days: 20

# keep similar job postings
similar: False

# skip web-scraping and load a previously saved daily scrape pickle
no_scrape: False

# recover master-list by accessing all historic scrapes pickles
recover: False

# saves duplicates removed by tfidf filter to duplicate_list.csv
save_duplicates: False

# delaying algorithm configuration
delay_config:
    # functions used for delaying algorithm, options are: constant, linear, sigmoid
    function: 'linear'
    # maximum delay/upper bound for converging random delay
    delay: 10.0
    # minimum delay/lower bound for random delay
    min_delay: 1.0
    # random delay
    random: False
    # converging random delay, only used if 'random' is set to True
    converge: False

# proxy settings
# proxy:
#   # protocol (http or https)
#   protocol: 'https'
#   # ip address
#   ip_address: '1.1.1.1'
#   # port
#   port: '200'
